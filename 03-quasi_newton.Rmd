---
output:
  pdf_document: default
  html_document: default
---
# Quasi Newton
Recall that the newton method uses the hessian to minimize the objective function.
On the contrary quasi-Newton methods uses an approximation of the true Hessian.
Therefore these method are a good alternative when the Hessian is unavailable or just to expensive to compute.
We could state it more formally in the following way; line search iterations are given by $x_{k + 1} = x_k + \alpha_k p_k$ where the search direction is given by $p_k = -B_k^{-1}\nabla f_k$.
In the case of steepest descent $B_k = I$, for Newton's method $B_k = \nabla^2 f_k$ and for quasi-Newton methods $B_k$ is an approximation to the Heassian that is updated at every iteration by means of a low-rank formula.

## BFGS
The BFGS algorithm is the most popular of the quasi-Newton methods.
First we will state the quadratic model of the objective function we will use at any given iteration
\begin{align}
  m_k(p) = f_k + \nabla f_k^{\top}p + \frac{1}{2}p^{\top}B_kp,
\end{align}
here $B_k$ is a symmetric and positive definite matrix that will be updated at every iteration.
The minimizer $p_k$ of this convex quadratic model, which we can write explicitly as
\begin{align}
  p_k = -B_k^{-1}\nabla f_k
\end{align}
is used as the search direction and the new iterate is
\begin{align}
  x_{k + 1} = x_k + \alpha_k p_k,
\end{align}
where $\alpha_k$ is the step length.

Computing $B_k$ at each iteration is expensive, therefore we update it iteratively using the curvature from the most recent step.
At a new iterate $x_{k + 1}$ construct a new quadratic model of the form
\begin{align}
  m_{k + 1}(p) = f_{k + 1} + \nabla f_{k + 1}^{\top} + \frac{1}{2}p^{\top}B_kp.
\end{align}
A reasonable requirement to $B_{k + 1}$, based on the knowledge gained during the latest step, is that $\nabla m_{k + 1}(p) = \nabla f_{k + 1} + B_{k + 1}p$ at both $x_k$ and $x_{k + 1}$.
As $\nabla m_{k + 1}(0) = \nabla f_{k + 1}$, the second condition is always satisfied.
The first condition can be written as
\begin{align}
  \nabla m_{k + 1}(-\alpha_k p_k) = \nabla f_{k + 1} - \alpha_k B_{k + 1} p_k = \nabla f_k.
\end{align}
By rearranging, we obtain
\begin{align}\label{eq:first_condition}
  B_{k + 1}\alpha_kp_k=\nabla f_{k + 1} - \nabla f_k,
\end{align}
to simplify the expression we introduce the notation
\begin{align}
  s_k = x_{k + 1} - x_k = \alpha p_k, \quad y_k = \nabla f_{k + 1} - \nabla f_k.
\end{align}
This lets us rewrite \eqref{eq:first_condition} as
\begin{align}\label{first_condition_alternative}
  B_{k + 1}s_k = y_k.
\end{align}
This formula is also known as the secant equation.
In other words we can say that the symmetric positive definite matrix $B_{k + 1}$ must map $s_k$ into $y_k$.
This is only possible if $s_k$ and $y_k$ satisfy the curvature condtion $s_k^{\top}y_k > 0$ as we have $s_k^{\top}B_{k + 1}s_k = s_k^{\top}y_k$ by multiplying \eqref{first_condition_alternative} with $s_k^{\top}$.
The equation is satisfied when $f$ is strongly convex, for any two $x_k$ and $x_{k + 1}$.
This will not always hold for nonconvex functions but can be enforced explicitly by choosing a step length $\alpha$ that satisfy the Wolfe or Strong Wolfe conditions.
When the curvature condition is met, the secant equation will have an infinite number of solution.
To choose $B_{k+1}$ uniquely we add the additional constraint: Among all of the symmetric matrices satisfying the secant equation, choose the $B_{k + 1}$ closest to the current matrix $B_k$.

The additional constraint corresponds to solving the problem
\begin{align}\label{eq:DFP_problem}
  \underset{B}{\min}\|B - B_k\|,
\end{align}
subject to
\begin{align}
  B = B^\top, \quad B_{s_k} = y_k,
\end{align}
where $s_k$ satisfies the secant equation and $B_k$ is symmetric and positive definite.

The BFGS updating formula can be derived with just a slight modification in the above argumentation, instead of imposing conditions on the Hessian approximations $B_k$, we impose similar conditions on their inverses giving by $H_k$.
The updated approximation must be symmetric and positive definite, and must satisfy the secant equation, now written as
\begin{align}
  H_{k + 1}y_k = s_k.
\end{align}
The condition of closeness to $H_k$ is now specified by the follwoing analogue of \eqref{eq:DFP_problem}
\begin{align}
  &\underset{H}{\min}\|H - H_k\|, \\
  &\text{subject to} \quad H = H^\top, \quad H_{y_k} = s_k.
\end{align}
A norm that allows easy solution of the minimization problem is the weighted Frobenius norm
\begin{align}
  \|A\|_W = \|W^{1/2}AW^{1/2}\|_F \quad \text{with the defintion of $\|\cdot\|_F$ as } \|C\|^2_F = \sum_{i = 1}^n\sum_{j = 1}^nc_{ij}^2. 
\end{align}
Note that $W$ can be chosen as any matrix satisfying the equation $Ws_k = y_k$.

The unique solution $H_{k + 1}$ to the minimization problem is given by
\begin{align}
  \text{(BFGS)} \qquad H_{k + 1} = (I - \rho_ks_ky_k^\top)H_k(I - \rho_ky_ks_k^\top) + \rho_ks_ks_k^\top,
\end{align}
with $\rho_k$ defined as
\begin{align}
  \rho_k = \frac{1}{y_k^\top s_k}.
\end{align}

### Choosing the initial approximation
There is no magic formula to choose the initial approximation $H_0$.
One could choose to make a finite difference approximation of the Hessian and then invert this approximation.
Another approach is just to set it equal to the identity matrix, or a multiple of the identity that reflects the scaling of the problem.

## Implementation of BFGS
Below is an implementation of the BFGS method, the initial approximation $H_0$ is set to the identity matrix and the step length is set to be $1$.
Although it could be beneficial to choose a step length that satisfy the Wolfe or Strong Wolfe conditions as mentioned above, we will see that BFGS will work even if the conditions are not satisfied.
Firstly we will define an objective function to test the BFGS on, we will use a convex elliptical objective function.

```{r objective_function}
# to determine convergence
norm2 <- function(x) norm(as.matrix(x), type = "2")

# objective function
f <- function(x) 0.5*(100*x[1]^2 + x[2]^2)
f_xy <- function(x,y) 0.5*(100*x^2 + y^2)
x_min_true <- c(0, 0)

# analytic derivatives
g <- function(x) c(0.5*200*x[1], 0.5*2*x[2])
H <- function(x) 0.5*rbind(c(200, 0), c(0, 2))
```

Now we will implement the BFGS method

```{r BFGS_method}
BFGS <- function(x_0) {
  x_k <- x_0
  tolerance <- 1e-10
  I <- rbind(c(1,0),c(0,1))
  h_k <- I
  k <- 0
  k_max <- 10000
  while (norm2(g(x_k)) >= tolerance & k < k_max) {
    x_k_old <- x_k
    g_k <- g(x_k)
    g_k_old <- g(x_k_old)
    p_k <- -h_k %*% g_k
    a_k <- 1
    x_k <- x_k + a_k * p_k
    g_k <- g(x_k)
    s_k <- x_k - x_k_old
    y_k <- g_k - g_k_old
    rho_k <- as.numeric(1/(t(y_k)%*%s_k))
    h_k <- {(I - rho_k * s_k %*% t(y_k)) %*% h_k %*% 
      (I - rho_k * y_k %*% t(s_k)) + rho_k * s_k %*% t(s_k)}
    k <- k + 1
  }
  cat('number of iterations:', '\t',  k, '\n')
  return(x_k)
}
```

We can now give an initial value and try the BFGS function

```{r comparing_methods}
x_0 <- c(1,1.5)
BFGS(x_0)
```

The BFGS method reaches convergence after 4 iterations.
Comparing this to the earlier implementation of gradient descent gives

```{r}
strong_wolfe_gd(f,g,x_0,tol_grad_f = 1e-10)
```

Thus we can conclude that on this problem the BFGS uses considerably less iterations, even with a step length not satisfiying the the Wolfe or strong Wolfe conditions.

```{r include = F}
library(microbenchmark)
bench <- microbenchmark(BFGS = BFGS(x_0),
                        GD = strong_wolfe_gd(f,g,x_0,tol_grad_f = 1e-10, output = FALSE),
                        times = 10)
library(dplyr)
bench <- bench %>%
  group_by(expr) %>%
  summarise(mean_time = mean(time)/1e+4)
```

Using `microbenchmark` we can see how much time it took to perform the iterations.
As every iteration in the BFGS algorithm is more expensive than in the gradient descent, the latter might still be faster.
However as we can see in the following plot, this is not the case.

```{r plotting_the_results_BFGS, echo = F, fig.height=3, fig.width=8}
library(ggplot2)
ggplot(bench) + 
  geom_col(aes(x = expr, y = mean_time), width = 0.5, fill = 'darkblue') +
  scale_y_continuous(trans = 'log10') +
  xlab('') + ylab('Mean time (log10 scale)') + ggtitle('Comparing test results') +
  theme_minimal()
```

### Inverse Hessian approximations
In this section we will investigate how well the Hessian is approximated in the BFGS algorithm.
The following `chunk` is a modified version of the BFGS implementation from earlier.
It has the added functionality that it saves the analytical Hessian and the approximated Hessian for each iteration.

```{r}
BFGS2 <- function(x_0, print = TRUE) {
  x_k <- x_0
  tolerance <- 1e-10
  I <- rbind(c(1,0),c(0,1))
  h_k <- I
  k <- 0
  k_max <- 10000
  #Create empty lists for matricies
  Hessian <- c()
  Truehessian <- c()
  while (norm2(g(x_k)) >= tolerance & k < k_max) {
    x_k_old <- x_k
    g_k <- g(x_k)
    g_k_old <- g(x_k_old)
    p_k <- -h_k %*% g_k
    a_k <- 1
    x_k <- x_k + a_k * p_k
    g_k <- g(x_k)
    s_k <- x_k - x_k_old
    y_k <- g_k - g_k_old
    rho_k <- as.numeric(1/(t(y_k)%*%s_k))
    h_k <- {(I - rho_k * s_k %*% t(y_k)) %*% h_k %*% 
        (I - rho_k * y_k %*% t(s_k)) + rho_k * s_k %*% t(s_k)}
    k <- k + 1
    Truehessian[[k]] <- solve(H(x_k))
    Hessian[[k]] <- h_k
    if (print == TRUE) {
      cat('Approximated inverse hessian:' , '\t' , h_k , '\n')
      cat('Analytical inverse hessian:' , '\t' , solve(H(x_k)) , '\n')  
    }
  }
  Hes_list <- list(Hessian, Truehessian)
  if (print == TRUE) {
    cat('number of iterations:', '\t',  k, '\n')
  }
  return(Hes_list)
}
Hes_list2 <- BFGS2(c(1,2))
```

The following plot shows the difference for each entry between the approximated and analytical Hessian.

```{r echo = F, fig.height=5, fig.width=8}
indgang.1.1 <- rep(NA_real_, 4)
indgang.1.2 <- rep(NA_real_, 4)
indgang.2.1 <- rep(NA_real_, 4)
indgang.2.2 <- rep(NA_real_, 4)
for (i in 1:4) {
  indgang.1.1[i] <- abs(BFGS2(c(1,2), 
                      print = F)[[2]][[i]][1,1] - BFGS2(c(1,2), print = F)[[1]][[i]][1,1])
  indgang.1.2[i] <- abs(BFGS2(c(1,2), 
                      print = F)[[2]][[i]][1,2] - BFGS2(c(1,2), print = F)[[1]][[i]][1,2])
  indgang.2.1[i] <- abs(BFGS2(c(1,2), 
                      print = F)[[2]][[i]][2,1] - BFGS2(c(1,2), print = F)[[1]][[i]][2,1])
  indgang.2.2[i] <- abs(BFGS2(c(1,2), 
                      print = F)[[2]][[i]][2,2] - BFGS2(c(1,2), print = F)[[1]][[i]][2,2])
}
par(mfrow=c(2,2))
plot(1:4, indgang.1.1, type = 'l', xlab = '')
plot(1:4, indgang.1.2, type = 'l', xlab = '')
plot(1:4, indgang.2.1, type = 'l', xlab = '')
plot(1:4, indgang.2.2, type = 'l', xlab = '')
```

They are very similar, even from the first iteration. 
The approximation is therefore practically identical to the analytical Hessian.
For 3 of the entries the difference even shrinks after the 4 iterations.

## Nelder-Mead
A popular derivative free optimization method is the Nelder-Mead method.
At any given point it keeps track of $n + 1$ points of interest in $\mathbb{R}^n$.
In a single iteration of the Nelder-Mead algorithm we seek to remove the point with the worst function value and replace it with another point with a better value.
The new point is found be either reflecting, expanding or contracting the simplex along the line joining the worst vertex with the centroid of the remaining vertices.
If we cannot find a better point in this manner, we retain only the vertex with the best function value, and we shrink the simplex by moving all other vertices toward this value.

We can now compare that to the `optim` functions implementation of 'Nelder-Mead', which is a derivative free optimisation algorithm with their implementation of BFGS.

```{r include = F}
library(microbenchmark)
bench <- microbenchmark(BFGS = optim(x_0, f, method = 'BFGS'),
                        NM = optim(x_0, f, method = 'Nelder-Mead'),
                        times = 50)
library(dplyr)
bench <- bench %>%
  group_by(expr) %>%
  summarise(mean_time = mean(time)/1e+4)
```

```{r plotting_the_results_BFGS_NM, echo = F, fig.height=3, fig.width=8}
library(ggplot2)
ggplot(bench) + 
  geom_col(aes(x = expr, y = mean_time), width = 0.5, fill = 'darkblue') +
  scale_y_continuous() +
  xlab('') + ylab('Mean time') + ggtitle('Comparing test results') +
  theme_minimal()
```

We conclude that the BFGS algorithm is faster than Nelder-Mead, however Nelder-Mead has the advantage that we don't have to provide the gradient.

\newpage