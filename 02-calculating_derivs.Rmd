# Calculating derivatives
Many numerical optimization algorithms use the gradient to minimize the respective objective function.
However sometimes it can be time-consuming if the objective function is complicated.
Therefore it would beneficial to have the algortihm calculate the derivative automatically.
There are several different approaches that could be taken.
The first approach we will consider is called finite differencing.

## Finite differencing
The idea is to estimate the derivatives by observing the change in function values in response to small perturbations of the unknowns near a given point.

### Foward difference
In the case of forward-difference our estimate of the gradient for a function is given by
\begin{align}
  \frac{\partial f}{\partial x_i}(x) \approx \frac{f(x + \varepsilon e_i)}{\varepsilon}.
\end{align}
In the case of central-difference our estimate is given as
\begin{align}
  \frac{\partial f}{\partial x_i}(x) \approx \frac{f(x + \varepsilon e_i) - f(x - \varepsilon e_i)}{2\varepsilon}.
\end{align}
These estimate arises from Taylor's formula that states
\begin{align}
  f(x + h) &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + \frac{1}{6}h^3f'''(x) - O(h^4),
\end{align}
rearranging gives that
\begin{align}
  f'(x) &= \frac{f(x + h) - f(x)}{h} + \frac{1}{2}hf''(x) + \frac{1}{6}h^2f'''(x) - O(h^3).
\end{align}
By considering only the first term after the equality we give rise to a truncation error.
We are only interested in $0 < h < 1$, such that the truncation error becomes $O(h)$.
Furthermore the truncation error decreases as $h$ decreases.
The could lead us to believe that we can then choose a very small $h$ a go on our merry way.
However this is not the case due to floating point arithmetic producing a round-off error.
It can be shown that the round-off error bound is $O(\varepsilon_M/h)$, i.e. the round-off error increases as $h$ decreases.
Thus we have established that there is a trade-off between truncation and round-off error.

### Central difference
Consider now both forward and backward difference, giving by
\begin{align}
  f(x + h) &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + O(h^3) \\
  f(x - h) &= f(x) - hf'(x) + \frac{1}{2}h^2f''(x) + O(h^3),
\end{align}
subtracting these two equations leads us to the following expression
\begin{align}
  f'(x) = \frac{f(x + h) - f(x - h)}{2h} - O(h^2).
\end{align}
In other words the truncation error is now smaller, however we need to evaluate the objective function twice.
This will have an impact on the performance of the algorithm, that depends on how expensive the objective function is to evaluate.

The difference between the estimates from the two methods can be seen in the following plot.

```{r comparing_the_two_methods, echo=F, fig.height=4, fig.width=8}
f <- function(x) sin(x)
g <- function(x) cos(x)
fd <- function(x){
  h <- 1e-1
  (f(x + h) - f(x))/h
}
cd <- function(x){
  h <- 1e-1
  (f(x + h) - f(x - h))/(2*h)
}
int <- seq(-0.3,0.3, 0.01)
curve(g, min(int), max(int))
lines(int, fd(int), col = 'blue')
lines(int, cd(int), col = 'green')
legend('topright', legend = c('cos(x)', 'FD', 'CD'),
       col = c('black', 'blue', 'green'), 
       lty = 1, cex = 0.8)
```

It is clear to see that the CD is closer to the actual gradient than the FD is.

## Five-Point stencil
The performance for the three different methods can be seen in the plot below.

```{r comparing_the_three_methods, echo=F, fig.height=4, fig.width=8}
h <- 0.1
f <- function(x) {sin(x)}
g_exact <- function(x) {cos(x)}
FD <- function(x) {
  FDf <- (f(x + h) - f(x))/h    
  return(FDf)
}
CD <- function(x) {
  CDf <- (f(x + h) - f(x - h))/(2*h)
  return(CDf)
}
XCD <- function(x) {
  XCDf <- (8*f(x + h) - 8*f(x - h) - f(x + 2*h) + f(x - 2*h))/(12*h)
  return(XCDf)
}

int <- seq(-.5,.5, 0.001)

diff_XCD <- function(x) g_exact(x) - XCD(x)
diff_CD <- function(x) g_exact(x) - CD(x)
diff_FD <- function(x) g_exact(x) - FD(x)
curve(diff_FD, min(int), max(int), ylab = "Difference", xlab = "")
lines(int, diff_CD(int), col = 'blue')
lines(int, diff_XCD(int), col = 'green')
legend("topleft", legend = c("FD", "CD", "Five-Point"),
       col = c("black", "blue", "green"),
       lty = 1, cex = 0.8)
```

## Finite difference gradient descent algortihm
We have implemented four different algortihms that utilizes the exact gradient, forward differencing, central difference and the five point stencil respectively.

```{r gradient_descent_algorithms, include=F}
# Exact functions
f <- function(x) 100*(x[2] - x[1]^2)^2 + (1 - x[1])^2
g <- function(x) c(2*(x[1] - 1) - 400*x[1]*(x[2] - x[1]^2), 200*(x[2] - x[1]^2))
# Implementation of Algorithm 3.5
# (Line Search Algorithm)
alpha <- function(a_0, x_k, c1, c2, r_k) {
  a_max <- 4*a_0
  f_k <- f(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  k <- 0
  k_max <- 10000   
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- f(x_k)
    g_k <- g(x_k)
    p_k <- -g_k
    phi_k_old <- f(x_k + a_k_old * p_k)
      phi_k <- f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <- f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
        return(zoom(a_k_old, a_k, x_k, c1, c2))
      }
    dphi_k <- t(g(x_k + a_k * p_k)) %*% p_k
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
    if (dphi_k >= 0) {
      return(zoom(a_k, a_k_old, x_k, c1, c2))
    }
    a_k_old <- a_k
    a_k <- r_k*a_k + (1 - r_k)*a_max
    done <- (k > k_max)
  }
  return(a_k)
}

# Implementation of Algorithm ...
# (Zoom Algorithm)
zoom <- function(a_lo, a_hi, x_k, c1, c2) {
  f_k <- f(x_k)
  g_k <- g(x_k)
  p_k <- -g_k
  k <- 0
  k_max <- 10000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    phi_lo <- f(x_k + a_lo * p_k)
      a_k <- 0.5*(a_lo + a_hi)
      phi_k <-  f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <-  f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || (phi_k >= phi_lo)) {
        a_hi <- a_k
      } else {
        dphi_k <-  t(g(x_k + a_k * p_k)) %*% p_k
          if (abs(dphi_k) <= -c2*dphi_k_0) {
            return(a_k)
          }
        if (dphi_k*(a_hi - a_lo) >= 0) {
          a_hi <- a_lo
        }
        a_lo <- a_k
      }
    done <- (k > k_max)
  }
  return(a_k)
}
# Implementation of Algorithm 3.5 (with CD instead of exact gradient)
# (Line Search Algorithm)
alpha_cd <- function(a_0, x_k, c1, c2, r_k) {
  g <- function(x, h = 1e-8){
    c((f(x + c(h,0)) - f(x - c(h,0)))/(2*h), (f(x + c(0,h)) - f(x - c(0,h)))/(2*h))
  }
  a_max <- 4*a_0
  f_k <- f(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  k <- 0
  k_max <- 10000   
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- f(x_k)
    g_k <- g(x_k)
    p_k <- -g_k
    phi_k_old <- f(x_k + a_k_old * p_k)
      phi_k <- f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <- f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
        return(zoom_cd(a_k_old, a_k, x_k, c1, c2))
      }
    dphi_k <- t(g(x_k + a_k * p_k)) %*% p_k
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
    if (dphi_k >= 0) {
      return(zoom_cd(a_k, a_k_old, x_k, c1, c2))
    }
    a_k_old <- a_k
    a_k <- r_k*a_k + (1 - r_k)*a_max
    done <- (k > k_max)
  }
  return(a_k)
}

# Implementation of Algorithm ... (with CD instead of exact gradient)
# (Zoom Algorithm)
zoom_cd <- function(a_lo, a_hi, x_k, c1, c2) {
  g <- function(x, h = 1e-8){
    c((f(x + c(h,0)) - f(x - c(h,0)))/(2*h), (f(x + c(0,h)) - f(x - c(0,h)))/(2*h))
  }
  f_k <- f(x_k)
  g_k <- g(x_k)
  p_k <- -g_k
  k <- 0
  k_max <- 10000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    phi_lo <- f(x_k + a_lo * p_k)
      a_k <- 0.5*(a_lo + a_hi)
      phi_k <-  f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <-  f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || (phi_k >= phi_lo)) {
        a_hi <- a_k
      } else {
        dphi_k <-  t(g(x_k + a_k * p_k)) %*% p_k
          if (abs(dphi_k) <= -c2*dphi_k_0) {
            return(a_k)
          }
        if (dphi_k*(a_hi - a_lo) >= 0) {
          a_hi <- a_lo
        }
        a_lo <- a_k
      }
    done <- (k > k_max)
  }
  return(a_k)
}

alpha_fd <- function(a_0, x_k, c1, c2, r_k) {
  g <- function(x, h = 1e-8){
    c((f(x + c(h,0)) - f(x))/(h), (f(x + c(0,h)) - f(x))/(h))
  }
  a_max <- 4*a_0
  f_k <- f(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  k <- 0
  k_max <- 10000   
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- f(x_k)
    g_k <- g(x_k)
    p_k <- -g_k
    phi_k_old <- f(x_k + a_k_old * p_k)
      phi_k <- f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <- f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
        return(zoom_fd(a_k_old, a_k, x_k, c1, c2))
      }
    dphi_k <- t(g(x_k + a_k * p_k)) %*% p_k
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
    if (dphi_k >= 0) {
      return(zoom_fd(a_k, a_k_old, x_k, c1, c2))
    }
    a_k_old <- a_k
    a_k <- r_k*a_k + (1 - r_k)*a_max
    done <- (k > k_max)
  }
  return(a_k)
}

# Implementation of Algorithm ... (with CD instead of exact gradient)
# (Zoom Algorithm)
zoom_fd <- function(a_lo, a_hi, x_k, c1, c2) {
  g <- function(x, h = 1e-8){
    c((f(x + c(h,0)) - f(x))/(h), (f(x + c(0,h)) - f(x))/(h))
  }
  f_k <- f(x_k)
  g_k <- g(x_k)
  p_k <- -g_k
  k <- 0
  k_max <- 10000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    phi_lo <- f(x_k + a_lo * p_k)
      a_k <- 0.5*(a_lo + a_hi)
      phi_k <-  f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <-  f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || (phi_k >= phi_lo)) {
        a_hi <- a_k
      } else {
        dphi_k <-  t(g(x_k + a_k * p_k)) %*% p_k
          if (abs(dphi_k) <= -c2*dphi_k_0) {
            return(a_k)
          }
        if (dphi_k*(a_hi - a_lo) >= 0) {
          a_hi <- a_lo
        }
        a_lo <- a_k
      }
    done <- (k > k_max)
  }
  return(a_k)
}

alpha_fp <- function(a_0, x_k, c1, c2, r_k) {
  g <- function(x, h = 1e-8){
    c((8*f(x + c(h,0)) - 8*f(x - c(h,0)) - f(x + c(2*h,0)) + f(x - c(2*h,0)))/(12*h),
      (8*f(x + c(0,h)) - 8*f(x - c(0,h)) - f(x + c(0,2*h)) + f(x - c(0,2*h)))/(12*h))
  }
  a_max <- 4*a_0
  f_k <- f(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  k <- 0
  k_max <- 10000   
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- f(x_k)
    g_k <- g(x_k)
    p_k <- -g_k
    phi_k_old <- f(x_k + a_k_old * p_k)
      phi_k <- f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <- f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
        return(zoom_fp(a_k_old, a_k, x_k, c1, c2))
      }
    dphi_k <- t(g(x_k + a_k * p_k)) %*% p_k
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
    if (dphi_k >= 0) {
      return(zoom_fp(a_k, a_k_old, x_k, c1, c2))
    }
    a_k_old <- a_k
    a_k <- r_k*a_k + (1 - r_k)*a_max
    done <- (k > k_max)
  }
  return(a_k)
}

# Implementation of Algorithm ... (with CD instead of exact gradient)
# (Zoom Algorithm)
zoom_fp <- function(a_lo, a_hi, x_k, c1, c2) {
  g <- function(x, h = 1e-8){
    c((8*f(x + c(h,0)) - 8*f(x - c(h,0)) - f(x + c(2*h,0)) + f(x - c(2*h,0)))/(12*h),
      (8*f(x + c(0,h)) - 8*f(x - c(0,h)) - f(x + c(0,2*h)) + f(x - c(0,2*h)))/(12*h))
  }
  f_k <- f(x_k)
  g_k <- g(x_k)
  p_k <- -g_k
  k <- 0
  k_max <- 10000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    phi_lo <- f(x_k + a_lo * p_k)
      a_k <- 0.5*(a_lo + a_hi)
      phi_k <-  f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <-  f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || (phi_k >= phi_lo)) {
        a_hi <- a_k
      } else {
        dphi_k <-  t(g(x_k + a_k * p_k)) %*% p_k
          if (abs(dphi_k) <= -c2*dphi_k_0) {
            return(a_k)
          }
        if (dphi_k*(a_hi - a_lo) >= 0) {
          a_hi <- a_lo
        }
        a_lo <- a_k
      }
    done <- (k > k_max)
  }
  return(a_k)
}
# Define norm to determine convergence
norm2 <- function(x) norm(as.matrix(x), type = "2")

# The base algorithm that uses the exact gradient
gd.strong.wolfe <- function(f, g, x_0, a_0 = 1, r_k = 0.5, c1 = 1e-4, c2 = 4e-1, tol_grad_f = 5e-4, k_max = 10000) {
  keep_going <- TRUE
  x_k <- x_0
  k <- 0
  while (keep_going) {
    k <- k + 1
    a_k <- alpha(a_0, x_k, c1, c2, r_k)
    g_k <- g(x_k)
    p_k <- -g_k
    x_k <- x_k + a_k*p_k
    keep_going <- (norm2(g_k) >= tol_grad_f) & (k < k_max)
  }
  #cat('x_k =', x_k, '\t', 'f(x_k) =', f(x_k), '\t', 'k =', k, '\n')
}
gd.strong.wolfe.FD <- function(f, g, x_0, a_0 = 1, r_k = 0.5, c1 = 1e-4, c2 = 4e-1, tol_grad_f = 5e-4, k_max = 10000) {
  keep_going <- TRUE
  x_k <- x_0
  k <- 0
  while (keep_going) {
    k <- k + 1
    a_k <- alpha_fd(a_0, x_k, c1, c2, r_k)
    g_k <- g(x_k)
    p_k <- -g_k
    x_k <- x_k + a_k*p_k
    keep_going <- (norm2(g_k) >= tol_grad_f) & (k < k_max)
  }
  #cat('x_k =', x_k, '\t', 'f(x_k) =', f(x_k), '\t', 'k =', k, '\n')
}
gd.strong.wolfe.CD <- function(f, g, x_0, a_0 = 1, r_k = 0.5, c1 = 1e-4, c2 = 4e-1, tol_grad_f = 5e-4, k_max = 10000) {
  keep_going <- TRUE
  x_k <- x_0
  k <- 0
  while (keep_going) {
    k <- k + 1
    a_k <- alpha_cd(a_0, x_k, c1, c2, r_k)
    g_k <- g(x_k)
    p_k <- -g_k
    x_k <- x_k + a_k*p_k
    keep_going <- (norm2(g_k) >= tol_grad_f) & (k < k_max)
  }
  #cat('x_k =', x_k, '\t', 'f(x_k) =', f(x_k), '\t', 'k =', k, '\n')
}
gd.strong.wolfe.FP <- function(f, g, x_0, a_0 = 1, r_k = 0.5, c1 = 1e-4, c2 = 4e-1, tol_grad_f = 5e-4, k_max = 10000) {
  keep_going <- TRUE
  x_k <- x_0
  k <- 0
  while (keep_going) {
    k <- k + 1
    a_k <- alpha_fp(a_0, x_k, c1, c2, r_k)
    g_k <- g(x_k)
    p_k <- -g_k
    x_k <- x_k + a_k*p_k
    keep_going <- (norm2(g_k) >= tol_grad_f) & (k < k_max)
  }
  #cat('x_k =', x_k, '\t', 'f(x_k) =', f(x_k), '\t', 'k =', k, '\n')
}
```

```{r comparing_the_algortihms, include = F}
library(microbenchmark)
bench <- microbenchmark(Exact = gd.strong.wolfe(f,g,c(0,1)),
               FD = gd.strong.wolfe.FD(f,g,c(0,1)),
               CD = gd.strong.wolfe.CD(f,g,c(0,1)),
               FP = gd.strong.wolfe.FP(f,g,c(0,1)),
               times = 10)
library(dplyr)
bench <- bench %>%
  group_by(expr) %>%
  summarise(mean_time = mean(time)/1e+6)
```

The following graph show how the different algorithms performed on average across 10 runs.

```{r plotting_the_results, echo = F, fig.height=3, fig.width=8}
library(ggplot2)
ggplot(bench) + 
  geom_col(aes(x = expr, y = mean_time), width = 0.5, fill = 'darkblue') +
  scale_y_continuous(breaks = seq(0,650,50)) +
  xlab('') + ylab('Mean time') + ggtitle('Comparing test results') +
  theme_minimal()
```

We can conclude that in this particular instance, we have the exact gradient giving the best performance.
From there the on the more complicated we make the finite differencing, the more performance we lose.
However this is only in this particular instance.
Suppose we have a very complicated objective function, in that case we could imagine that if we can make our gradient estimate more precise it would be worth the added computation invloved.

<!-- ### What would happen if we extend the central-difference to also use $f(x - 2h)$ and $f(x + 2h)$? Hint: consider the Taylor series up to a sufficiently high power of $h$. Hint: "five-point stencil". -->

<!-- First we deduce the five point stencil rule by Taylor expansion -->
<!-- $$f(x \pm h) = f(x) \pm h f'(x) + \frac{h^2}{2!} f''(x) \pm \frac{h^3}{3!} f'''(x) + O(h^4)$$ -->

<!-- $$f(x+h) - f(x-h) = 2 h f'(x) + \frac{h^3}{3} f'''(x) O(h^4)$$ -->
<!-- $$ f(x+2h) - f(x-2h) = 4h f'(x) + \frac{8 h^3}{3} f'''(x) + O(h^4)$$ -->
<!-- To get rid of the third term we write -->
<!-- $$8f(x+h) - 8f(x-h) - f(x+2h)+f(x-2h) = 12 h f'(x) + O(h^4)$$ -->
<!-- Thus we have -->
<!-- $$f'(x) = \frac{8 f(x+h) - 8f(x-h) - f(x+2h) + f(x-2h)}{12h}$$ -->
<!-- where our truncation error is $O(h^4)$, which all else equal is better than for both forward and central differencing.  -->
<!-- These are $O(h)$ and $O(h^2)$ respectively. -->

<!-- ### Analyse this method in comparison with FD and CD (theoretically and practically on specific examples) -->

<!-- ### What are the advantages and disadvantages of the different finite difference methods? -->
<!-- When we use more point to calculate the derivative we get a smaller truncation error thus allowing us to use a smaller value for $h$. -->
<!-- The disadvantage is that the function needs to be evaluated in more points which could be expensive. -->
<!-- Therefore the choice of method depends on the specific problem. -->

\newpage