# Calculating derivatives
Many numerical optimization algorithms use the gradient to minimize the respective objective function.
However sometimes it can be time-consuming if the objective function is complicated.
Therefore it would beneficial to have the algortihm calculate the derivative automatically.
There are several different approaches that could be taken.
The first approach we will consider is called finite differencing.

## Finite differencing
The idea is to estimate the derivatives by observing the change in function values in response to small perturbations of the unknowns near a given point.

### Foward difference
In the case of forward-difference our estimate of the gradient for a function is given by
\begin{align}
  \frac{\partial f}{\partial x_i}(x) \approx \frac{f(x + \varepsilon e_i)}{\varepsilon}.
\end{align}
In the case of central-difference our estimate is given as
\begin{align}
  \frac{\partial f}{\partial x_i}(x) \approx \frac{f(x + \varepsilon e_i) - f(x - \varepsilon e_i)}{2\varepsilon}.
\end{align}
These estimate arises from Taylor's formula that states
\begin{align}
  f(x + h) &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + \frac{1}{6}h^3f'''(x) - O(h^4),
\end{align}
rearranging gives that
\begin{align}
  f'(x) &= \frac{f(x + h) - f(x)}{h} + \frac{1}{2}hf''(x) + \frac{1}{6}h^2f'''(x) - O(h^3).
\end{align}
By considering only the first term after the equality we give rise to a truncation error.
We are only interested in $0 < h < 1$, such that the truncation error becomes $O(h)$.
Furthermore the truncation error decreases as $h$ decreases.
The could lead us to believe that we can then choose a very small $h$ a go on our merry way.
However this is not the case due to floating point arithmetic producing a round-off error.
It can be shown that the round-off error bound is $O(\varepsilon_M/h)$, i.e. the round-off error increases as $h$ decreases.
Thus we have established that there is a trade-off between truncation and round-off error.

### Central difference
Consider now both forward and backward difference, giving by
\begin{align}
  f(x + h) &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + O(h^3) \\
  f(x - h) &= f(x) - hf'(x) + \frac{1}{2}h^2f''(x) + O(h^3),
\end{align}
subtracting these two equations leads us to the following expression
\begin{align}
  f'(x) = \frac{f(x + h) - f(x - h)}{2h} - O(h^2).
\end{align}
In other words the truncation error is now smaller, however we need to evaluate the objective function twice.
This will have an impact on the performance of the algorithm, that depends on how expensive the objective function is to evaluate.

The difference between the estimates from the two methods can be seen in the following plot.

```{r echo=F, fig.height=4, fig.width=8}
f <- function(x) sin(x)
g <- function(x) cos(x)
fd <- function(x){
  h <- 1e-1
  (f(x + h) - f(x))/h
}
cd <- function(x){
  h <- 1e-1
  (f(x + h) - f(x - h))/(2*h)
}
int <- seq(-0.3,0.3, 0.01)
curve(g, min(int), max(int))
lines(int, fd(int), col = 'blue')
lines(int, cd(int), col = 'green')
legend('topright', legend = c('cos(x)', 'FD', 'CD'),
       col = c('black', 'blue', 'green'), 
       lty = 1, cex = 0.8)
```

It is clear to see that the CD is closer to the actual gradient than the FD is.

## Finite difference gradient descent algortihm
We can now present a gradient descent algorithm that uses the FD and CD approach.




## Five-Point stencil
The performance for the three different methods can be seen in the plot below.

```{r echo=F, fig.height=4, fig.width=8}
h <- 0.1
f <- function(x) {sin(x)}
g_exact <- function(x) {cos(x)}
FD <- function(x) {
  FDf <- (f(x + h) - f(x))/h    
  return(FDf)
}
CD <- function(x) {
  CDf <- (f(x + h) - f(x - h))/(2*h)
  return(CDf)
}
XCD <- function(x) {
  XCDf <- (8*f(x + h) - 8*f(x - h) - f(x + 2*h) + f(x - 2*h))/(12*h)
  return(XCDf)
}

int <- seq(-.5,.5, 0.001)

diff_XCD <- function(x) g_exact(x) - XCD(x)
diff_CD <- function(x) g_exact(x) - CD(x)
diff_FD <- function(x) g_exact(x) - FD(x)
curve(diff_FD, min(int), max(int), ylab = "Difference", xlab = "")
lines(int, diff_CD(int), col = 'blue')
lines(int, diff_XCD(int), col = 'green')
legend("topleft", legend = c("FD", "CD", "Five-Point"),
       col = c("black", "blue", "green"),
       lty = 1, cex = 0.8)
```

<!-- ### What would happen if we extend the central-difference to also use $f(x - 2h)$ and $f(x + 2h)$? Hint: consider the Taylor series up to a sufficiently high power of $h$. Hint: "five-point stencil". -->

<!-- First we deduce the five point stencil rule by Taylor expansion -->
<!-- $$f(x \pm h) = f(x) \pm h f'(x) + \frac{h^2}{2!} f''(x) \pm \frac{h^3}{3!} f'''(x) + O(h^4)$$ -->

<!-- $$f(x+h) - f(x-h) = 2 h f'(x) + \frac{h^3}{3} f'''(x) O(h^4)$$ -->
<!-- $$ f(x+2h) - f(x-2h) = 4h f'(x) + \frac{8 h^3}{3} f'''(x) + O(h^4)$$ -->
<!-- To get rid of the third term we write -->
<!-- $$8f(x+h) - 8f(x-h) - f(x+2h)+f(x-2h) = 12 h f'(x) + O(h^4)$$ -->
<!-- Thus we have -->
<!-- $$f'(x) = \frac{8 f(x+h) - 8f(x-h) - f(x+2h) + f(x-2h)}{12h}$$ -->
<!-- where our truncation error is $O(h^4)$, which all else equal is better than for both forward and central differencing.  -->
<!-- These are $O(h)$ and $O(h^2)$ respectively. -->

<!-- ### Analyse this method in comparison with FD and CD (theoretically and practically on specific examples) -->

<!-- ### What are the advantages and disadvantages of the different finite difference methods? -->
<!-- When we use more point to calculate the derivative we get a smaller truncation error thus allowing us to use a smaller value for $h$. -->
<!-- The disadvantage is that the function needs to be evaluated in more points which could be expensive. -->
<!-- Therefore the choice of method depends on the specific problem. -->

\newpage