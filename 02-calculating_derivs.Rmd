---
output:
  pdf_document: default
  html_document: default
---
# Calculating derivatives
Many numerical optimization algorithms use the gradient to minimize the respective objective function.
However sometimes it can be time consuming if the objective function is complicated.
Therefore it would beneficial to have the algortihm calculate the derivative automatically.
There are several different approaches that could be taken.
The first approach we will consider is called finite differencing.

## Finite differencing
The idea is to estimate the derivatives by observing the change in function values in response to small perturbations of the unknowns near a given point.

### Forward difference
In the case of forward-difference our estimate of the gradient for a function is given by
\begin{align}
  \frac{\partial f}{\partial x_i}(x) \approx \frac{f(x + \varepsilon e_i) - f(x)}{\varepsilon}.
\end{align}
In the case of central-difference our estimate is given as
\begin{align}
  \frac{\partial f}{\partial x_i}(x) \approx \frac{f(x + \varepsilon e_i) - f(x - \varepsilon e_i)}{2\varepsilon}.
\end{align}
These estimate arises from Taylor's formula that states
\begin{align}
  f(x + h) &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + \frac{1}{6}h^3f'''(x) + O(h^4),
\end{align}
rearranging gives that
\begin{align}
  f'(x) &= \frac{f(x + h) - f(x)}{h} + \frac{1}{2}hf''(x) + \frac{1}{6}h^2f'''(x) - O(h^3).
\end{align}
By considering only the first term after the equality we give rise to a truncation error.
We are only interested in $0 < h < 1$, such that the truncation error becomes $O(h)$.
Furthermore the truncation error decreases as $h$ decreases.


### Central difference
Consider now both forward and backward difference, giving by
\begin{align}
  f(x + h) &= f(x) + hf'(x) + \frac{1}{2}h^2f''(x) + O(h^3) \\
  f(x - h) &= f(x) - hf'(x) + \frac{1}{2}h^2f''(x) + O(h^3),
\end{align}
subtracting these two equations leads us to the following expression
\begin{align}
  f'(x) = \frac{f(x + h) - f(x - h)}{2h} - O(h^2).
\end{align}
In other words the truncation error is now smaller, however we need to evaluate the objective function twice.
This will have an impact on the performance of the algorithm, that depends on how expensive the objective function is to evaluate.

The difference between the estimates from the two methods can be seen in the following plot.

```{r comparing_the_two_methods, echo=F, fig.height=4, fig.width=8}
f <- function(x) sin(x)
g <- function(x) cos(x)
fd <- function(x){
  h <- 1e-1
  (f(x + h) - f(x))/h
}
cd <- function(x){
  h <- 1e-1
  (f(x + h) - f(x - h))/(2*h)
}
int <- seq(-0.3,0.3, 0.01)
curve(g, min(int), max(int))
lines(int, fd(int), col = 'blue')
lines(int, cd(int), col = 'green')
legend('topright', legend = c('cos(x)', 'FD', 'CD'),
       col = c('black', 'blue', 'green'), 
       lty = 1, cex = 0.8)
```

It is clear to see that CD is closer to the actual gradient than FD is.

### Five-Point stencil
Going further than the central difference formula, one could also include the terms $f(x + 2h)$ and $f(x - 2h)$ to cancel out even more terms.
Recalliung the taylor expansion of $f$ gives
$$f(x \pm h) = f(x) \pm h f'(x) + \frac{h^2}{2!} f''(x) \pm \frac{h^3}{3!} f'''(x) + O(h^4)$$
$$f(x+h) - f(x-h) = 2 h f'(x) + \frac{h^3}{3} f'''(x) + O(h^4)$$
$$ f(x+2h) - f(x-2h) = 4h f'(x) + \frac{8 h^3}{3} f'''(x) + O(h^4)$$
To get rid of the third term we write
$$8f(x+h) - 8f(x-h) - f(x+2h)+f(x-2h) = 12 h f'(x) + O(h^4)$$
Thus we have
$$f'(x) = \frac{8 f(x+h) - 8f(x-h) - f(x+2h) + f(x-2h)}{12h}$$
where our truncation error is $O(h^4)$, which all else equal is better than for both forward and central differencing.
These are $O(h)$ and $O(h^2)$ respectively.

<!-- The performance for the three different methods can be seen in the plot below. -->

<!-- ```{r comparing_the_three_methods, echo=F, fig.height=4, fig.width=8} -->
<!-- h <- 0.1 -->
<!-- f <- function(x) {sin(x)} -->
<!-- g_exact <- function(x) {cos(x)} -->
<!-- FD <- function(x) { -->
<!--   FDf <- (f(x + h) - f(x))/h     -->
<!--   return(FDf) -->
<!-- } -->
<!-- CD <- function(x) { -->
<!--   CDf <- (f(x + h) - f(x - h))/(2*h) -->
<!--   return(CDf) -->
<!-- } -->
<!-- XCD <- function(x) { -->
<!--   XCDf <- (8*f(x + h) - 8*f(x - h) - f(x + 2*h) + f(x - 2*h))/(12*h) -->
<!--   return(XCDf) -->
<!-- } -->

<!-- int <- seq(-.5,.5, 0.001) -->

<!-- diff_XCD <- function(x) g_exact(x) - XCD(x) -->
<!-- diff_CD <- function(x) g_exact(x) - CD(x) -->
<!-- diff_FD <- function(x) g_exact(x) - FD(x) -->
<!-- curve(diff_FD, min(int), max(int), ylab = "Difference", xlab = "") -->
<!-- lines(int, diff_CD(int), col = 'blue') -->
<!-- lines(int, diff_XCD(int), col = 'green') -->
<!-- legend("topleft", legend = c("FD", "CD", "Five-Point"), -->
<!--        col = c("black", "blue", "green"), -->
<!--        lty = 1, cex = 0.8) -->
<!-- ``` -->

## Error tradeoff
As we are using computers to aid us in calculating the finite differencing estimates we run into the problem of round off errors.
The roundoff error bound is given by
\begin{align*}
  O(\epsilon_M/h).
\end{align*}
This means that the roundoff error increases when $h$ decreases.
Thus we have established that there is a trade-off between truncation and round-off error.
In the following plot we can see the relationship between $h$ and the difference between the exact gradient and the estimate produced from the three methods.
The function in question is
\begin{align*}
  f(x) = \cos(\cos(x)\sin(x)),
\end{align*}
evaluated at $x = \frac{\pi}{3}$.

```{r echo=F, fig.height=4, fig.width=8}
x <- pi/3
h <- c(1e-01,1e-02,1e-03,1e-04,1e-05,1e-06,1e-07,1e-08,
       1e-09,1e-10,1e-11,1e-12,1e-13,1e-14,1e-15,1e-16)
f <- function(x) {cos(cos(x)*sin(x))}
g_exact <- function(x) {-sin(cos(x)*sin(x))*(-(sin(x)^2)+(cos(x)^2))}
FD <- function(x, h) {
  FDf <- (f(x + h) - f(x))/h    
  return(FDf)
}
CD <- function(x, h) {
  CDf <- (f(x + h) - f(x - h))/(2*h)
  return(CDf)
}
XCD <- function(x, h) {
  XCDf <- (8*f(x + h) - 8*f(x - h) - f(x + 2*h) + f(x - 2*h))/(12*h)
  return(XCDf)
}
plot(log10(h), log10(abs(XCD(x, h) - g_exact(x))), type = 'l', col = 'red', ylab = 'log10(Error)', xaxt = 'n')
axis(side = 1, at=seq(-16,0,2))
lines(log10(h), log10(abs(FD(x, h) - g_exact(x))), col = 'blue')
lines(log10(h), log10(abs(CD(x, h) - g_exact(x))), col = 'green')
legend("bottomleft", legend = c("FD", "CD", "Five-Point"),
       col = c("green", "blue", "red"),
       lty = 1, cex = 0.8)
```

## Finite difference gradient descent algorithm
We have implemented four different algortihms that utilizes the exact gradient, forward differencing, central difference and the five point stencil respectively.

```{r gradient_descent_algorithms, include=F}
# Exact functions
f <- function(x) 100*(x[2] - x[1]^2)^2 + (1 - x[1])^2
g <- function(x) c(2*(x[1] - 1) - 400*x[1]*(x[2] - x[1]^2), 200*(x[2] - x[1]^2))
# Implementation of Algorithm 3.5
# (Line Search Algorithm)
alpha <- function(a_0, x_k, c1, c2, r_k) {
  a_max <- 4*a_0
  f_k <- f(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  k <- 0
  k_max <- 10000   
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- f(x_k)
    g_k <- g(x_k)
    p_k <- -g_k
    phi_k_old <- f(x_k + a_k_old * p_k)
      phi_k <- f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <- f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
        return(zoom(a_k_old, a_k, x_k, c1, c2))
      }
    dphi_k <- t(g(x_k + a_k * p_k)) %*% p_k
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
    if (dphi_k >= 0) {
      return(zoom(a_k, a_k_old, x_k, c1, c2))
    }
    a_k_old <- a_k
    a_k <- r_k*a_k + (1 - r_k)*a_max
    done <- (k > k_max)
  }
  return(a_k)
}

# Implementation of Algorithm ...
# (Zoom Algorithm)
zoom <- function(a_lo, a_hi, x_k, c1, c2) {
  f_k <- f(x_k)
  g_k <- g(x_k)
  p_k <- -g_k
  k <- 0
  k_max <- 10000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    phi_lo <- f(x_k + a_lo * p_k)
      a_k <- 0.5*(a_lo + a_hi)
      phi_k <-  f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <-  f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || (phi_k >= phi_lo)) {
        a_hi <- a_k
      } else {
        dphi_k <-  t(g(x_k + a_k * p_k)) %*% p_k
          if (abs(dphi_k) <= -c2*dphi_k_0) {
            return(a_k)
          }
        if (dphi_k*(a_hi - a_lo) >= 0) {
          a_hi <- a_lo
        }
        a_lo <- a_k
      }
    done <- (k > k_max)
  }
  return(a_k)
}
# Implementation of Algorithm 3.5 (with CD instead of exact gradient)
# (Line Search Algorithm)
alpha_cd <- function(a_0, x_k, c1, c2, r_k) {
  g <- function(x, h = 1e-8){
    c((f(x + c(h,0)) - f(x - c(h,0)))/(2*h), (f(x + c(0,h)) - f(x - c(0,h)))/(2*h))
  }
  a_max <- 4*a_0
  f_k <- f(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  k <- 0
  k_max <- 10000   
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- f(x_k)
    g_k <- g(x_k)
    p_k <- -g_k
    phi_k_old <- f(x_k + a_k_old * p_k)
      phi_k <- f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <- f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
        return(zoom_cd(a_k_old, a_k, x_k, c1, c2))
      }
    dphi_k <- t(g(x_k + a_k * p_k)) %*% p_k
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
    if (dphi_k >= 0) {
      return(zoom_cd(a_k, a_k_old, x_k, c1, c2))
    }
    a_k_old <- a_k
    a_k <- r_k*a_k + (1 - r_k)*a_max
    done <- (k > k_max)
  }
  return(a_k)
}

# Implementation of Algorithm ... (with CD instead of exact gradient)
# (Zoom Algorithm)
zoom_cd <- function(a_lo, a_hi, x_k, c1, c2) {
  g <- function(x, h = 1e-8){
    c((f(x + c(h,0)) - f(x - c(h,0)))/(2*h), (f(x + c(0,h)) - f(x - c(0,h)))/(2*h))
  }
  f_k <- f(x_k)
  g_k <- g(x_k)
  p_k <- -g_k
  k <- 0
  k_max <- 10000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    phi_lo <- f(x_k + a_lo * p_k)
      a_k <- 0.5*(a_lo + a_hi)
      phi_k <-  f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <-  f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || (phi_k >= phi_lo)) {
        a_hi <- a_k
      } else {
        dphi_k <-  t(g(x_k + a_k * p_k)) %*% p_k
          if (abs(dphi_k) <= -c2*dphi_k_0) {
            return(a_k)
          }
        if (dphi_k*(a_hi - a_lo) >= 0) {
          a_hi <- a_lo
        }
        a_lo <- a_k
      }
    done <- (k > k_max)
  }
  return(a_k)
}

alpha_fd <- function(a_0, x_k, c1, c2, r_k) {
  g <- function(x, h = 1e-8){
    c((f(x + c(h,0)) - f(x))/(h), (f(x + c(0,h)) - f(x))/(h))
  }
  a_max <- 4*a_0
  f_k <- f(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  k <- 0
  k_max <- 10000   
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- f(x_k)
    g_k <- g(x_k)
    p_k <- -g_k
    phi_k_old <- f(x_k + a_k_old * p_k)
      phi_k <- f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <- f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
        return(zoom_fd(a_k_old, a_k, x_k, c1, c2))
      }
    dphi_k <- t(g(x_k + a_k * p_k)) %*% p_k
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
    if (dphi_k >= 0) {
      return(zoom_fd(a_k, a_k_old, x_k, c1, c2))
    }
    a_k_old <- a_k
    a_k <- r_k*a_k + (1 - r_k)*a_max
    done <- (k > k_max)
  }
  return(a_k)
}

# Implementation of Algorithm ... (with CD instead of exact gradient)
# (Zoom Algorithm)
zoom_fd <- function(a_lo, a_hi, x_k, c1, c2) {
  g <- function(x, h = 1e-8){
    c((f(x + c(h,0)) - f(x))/(h), (f(x + c(0,h)) - f(x))/(h))
  }
  f_k <- f(x_k)
  g_k <- g(x_k)
  p_k <- -g_k
  k <- 0
  k_max <- 10000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    phi_lo <- f(x_k + a_lo * p_k)
      a_k <- 0.5*(a_lo + a_hi)
      phi_k <-  f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <-  f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || (phi_k >= phi_lo)) {
        a_hi <- a_k
      } else {
        dphi_k <-  t(g(x_k + a_k * p_k)) %*% p_k
          if (abs(dphi_k) <= -c2*dphi_k_0) {
            return(a_k)
          }
        if (dphi_k*(a_hi - a_lo) >= 0) {
          a_hi <- a_lo
        }
        a_lo <- a_k
      }
    done <- (k > k_max)
  }
  return(a_k)
}

alpha_fp <- function(a_0, x_k, c1, c2, r_k) {
  g <- function(x, h = 1e-8){
    c((8*f(x + c(h,0)) - 8*f(x - c(h,0)) - f(x + c(2*h,0)) + f(x - c(2*h,0)))/(12*h),
      (8*f(x + c(0,h)) - 8*f(x - c(0,h)) - f(x + c(0,2*h)) + f(x - c(0,2*h)))/(12*h))
  }
  a_max <- 4*a_0
  f_k <- f(x_k)
  phi_k <- f_k
  a_1 <- a_0
  a0 <- 0
  a_k <- a_1
  a_k_old <- a0
  k <- 0
  k_max <- 10000   
  done <- FALSE
  while(!done) {
    k <- k + 1
    f_k <- f(x_k)
    g_k <- g(x_k)
    p_k <- -g_k
    phi_k_old <- f(x_k + a_k_old * p_k)
      phi_k <- f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <- f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || ((k > 1) && (phi_k >= phi_k_old))) {
        return(zoom_fp(a_k_old, a_k, x_k, c1, c2))
      }
    dphi_k <- t(g(x_k + a_k * p_k)) %*% p_k
      if (abs(dphi_k) <= -c2*dphi_k_0) {
        return(a_k)
      }
    if (dphi_k >= 0) {
      return(zoom_fp(a_k, a_k_old, x_k, c1, c2))
    }
    a_k_old <- a_k
    a_k <- r_k*a_k + (1 - r_k)*a_max
    done <- (k > k_max)
  }
  return(a_k)
}

# Implementation of Algorithm ... (with CD instead of exact gradient)
# (Zoom Algorithm)
zoom_fp <- function(a_lo, a_hi, x_k, c1, c2) {
  g <- function(x, h = 1e-8){
    c((8*f(x + c(h,0)) - 8*f(x - c(h,0)) - f(x + c(2*h,0)) + f(x - c(2*h,0)))/(12*h),
      (8*f(x + c(0,h)) - 8*f(x - c(0,h)) - f(x + c(0,2*h)) + f(x - c(0,2*h)))/(12*h))
  }
  f_k <- f(x_k)
  g_k <- g(x_k)
  p_k <- -g_k
  k <- 0
  k_max <- 10000   # Maximum number of iterations.
  done <- FALSE
  while(!done) {
    k <- k + 1
    phi_lo <- f(x_k + a_lo * p_k)
      a_k <- 0.5*(a_lo + a_hi)
      phi_k <-  f(x_k + a_k * p_k)
      dphi_k_0 <- t(g(x_k)) %*% p_k
      l_k <-  f_k + c1 * a_k * dphi_k_0
      if ((phi_k > l_k) || (phi_k >= phi_lo)) {
        a_hi <- a_k
      } else {
        dphi_k <-  t(g(x_k + a_k * p_k)) %*% p_k
          if (abs(dphi_k) <= -c2*dphi_k_0) {
            return(a_k)
          }
        if (dphi_k*(a_hi - a_lo) >= 0) {
          a_hi <- a_lo
        }
        a_lo <- a_k
      }
    done <- (k > k_max)
  }
  return(a_k)
}
# Define norm to determine convergence
norm2 <- function(x) norm(as.matrix(x), type = "2")

# The base algorithm that uses the exact gradient
gd.strong.wolfe <- function(f, g, x_0, a_0 = 1, r_k = 0.5, c1 = 1e-4, c2 = 4e-1, tol_grad_f = 5e-4, k_max = 10000) {
  keep_going <- TRUE
  x_k <- x_0
  k <- 0
  while (keep_going) {
    k <- k + 1
    a_k <- alpha(a_0, x_k, c1, c2, r_k)
    g_k <- g(x_k)
    p_k <- -g_k
    x_k <- x_k + a_k*p_k
    keep_going <- (norm2(g_k) >= tol_grad_f) & (k < k_max)
  }
  #cat('x_k =', x_k, '\t', 'f(x_k) =', f(x_k), '\t', 'k =', k, '\n')
}
gd.strong.wolfe.FD <- function(f, g, x_0, a_0 = 1, r_k = 0.5, c1 = 1e-4, c2 = 4e-1, tol_grad_f = 5e-4, k_max = 10000) {
  keep_going <- TRUE
  x_k <- x_0
  k <- 0
  while (keep_going) {
    k <- k + 1
    a_k <- alpha_fd(a_0, x_k, c1, c2, r_k)
    g_k <- g(x_k)
    p_k <- -g_k
    x_k <- x_k + a_k*p_k
    keep_going <- (norm2(g_k) >= tol_grad_f) & (k < k_max)
  }
  #cat('x_k =', x_k, '\t', 'f(x_k) =', f(x_k), '\t', 'k =', k, '\n')
}
gd.strong.wolfe.CD <- function(f, g, x_0, a_0 = 1, r_k = 0.5, c1 = 1e-4, c2 = 4e-1, tol_grad_f = 5e-4, k_max = 10000) {
  keep_going <- TRUE
  x_k <- x_0
  k <- 0
  while (keep_going) {
    k <- k + 1
    a_k <- alpha_cd(a_0, x_k, c1, c2, r_k)
    g_k <- g(x_k)
    p_k <- -g_k
    x_k <- x_k + a_k*p_k
    keep_going <- (norm2(g_k) >= tol_grad_f) & (k < k_max)
  }
  #cat('x_k =', x_k, '\t', 'f(x_k) =', f(x_k), '\t', 'k =', k, '\n')
}
gd.strong.wolfe.FP <- function(f, g, x_0, a_0 = 1, r_k = 0.5, c1 = 1e-4, c2 = 4e-1, tol_grad_f = 5e-4, k_max = 10000) {
  keep_going <- TRUE
  x_k <- x_0
  k <- 0
  while (keep_going) {
    k <- k + 1
    a_k <- alpha_fp(a_0, x_k, c1, c2, r_k)
    g_k <- g(x_k)
    p_k <- -g_k
    x_k <- x_k + a_k*p_k
    keep_going <- (norm2(g_k) >= tol_grad_f) & (k < k_max)
  }
  #cat('x_k =', x_k, '\t', 'f(x_k) =', f(x_k), '\t', 'k =', k, '\n')
}
```

```{r comparing_the_algortihms, include = F}
library(microbenchmark)
bench <- microbenchmark(Exact = gd.strong.wolfe(f,g,c(0,1)),
               FD = gd.strong.wolfe.FD(f,g,c(0,1)),
               CD = gd.strong.wolfe.CD(f,g,c(0,1)),
               FP = gd.strong.wolfe.FP(f,g,c(0,1)),
               times = 10)
library(dplyr)
bench <- bench %>%
  group_by(expr) %>%
  summarise(mean_time = mean(time)/1e+6)
library(ggplot2)
```

The following graph show how the different algorithms performed, on average, across 10 runs.

```{r plotting_the_results, echo = F, fig.height=3, fig.width=8}
ggplot(bench) + 
  geom_col(aes(x = expr, y = mean_time), width = 0.5, fill = 'darkblue') +
  xlab('') + ylab('Mean time') + ggtitle('Comparing test results') +
  theme_minimal()
```

We can conclude that in this particular instance, we have the exact gradient giving the best performance.
From there, the more complicated we make the finite differencing, the more performance we lose.
However this is only in this particular instance.
Suppose we have a very complicated objective function, in that case we could imagine that if we can make our gradient estimate more precise it would be worth the added computation invloved.

## Algorithmic differentiation (AD)
### Forward mode
Algorithmic differentiation builds on the principle that, no matter how complicated a function might be, it builds on a sequence of simple elementary operations.
To illustrate we will consider an example.
Take the function
\begin{align}
  f(x) = \cos(x_1 + x_2) \cdot x_3.
\end{align}
We can express the evaluation of $f$ as
\begin{align}
  x_4 &= x_1 + x_2 \\
  x_5 &= \cos(x_4) \\
  x_6 &= x_5 \cdot x_3,
\end{align}
with the final node $x_6$ being the function value $f(x)$.

Suppose we now want to find the derivative of this function w.r.t. $x_1$.
If we denote the intermediate variables by $v$, and to each variable associate a new variable, $\dot{v}_i = \partial v_i/\partial x_1$.
We can then apply the chain rule mechanically to each line in the evaluation trace, this will then give the numerical value to each $\dot{v}_i$.
First we will go thorugh the first iterations as an example, and then present the full evaluation trace
\begin{align*}
  v_{-2} &= x_1 &&= 1 \\
  v_{-1} &= x_2 &&= 2 \\
  v_{0} &= x_3 &&= 3 \\
  v_{1} &= v_{-2} + v_{-1} &&= 3 \\
  v_{2} &= \cos(v_1) &&= -0.99 \\
  v_{3} &= v_2 \cdot v_0 &&= -2.97 \\
  y &= v_3 &&= -2.97 
\end{align*}
Using the chain rule, given by
\begin{align}
  \frac{\partial y}{\partial x_i} = \sum_{j = 1}^{m} \frac{\partial y}{\partial u_j}\frac{\partial u_j}{\partial x_i}
\end{align}
we find that
\begin{align*}
  &\dot{v}_{-2} = \frac{\partial v_{-2}}{\partial x_1} = \frac{\partial}{\partial x_1}x_1 = 1, \quad \dot{v}_{-1} = \frac{\partial v_{-1}}{\partial x_1} = \frac{\partial}{\partial x_1}x_2 = 0 \quad \text{and} \quad \dot{v}_{0} = \frac{\partial v_{0}}{\partial x_1} = \frac{\partial}{\partial x_1}x_3 = 0 \\
  &\dot{v}_1 = \frac{\partial v_1}{\partial x_1} = 
  \frac{\partial v_1}{\partial v_{-2}}
  \frac{\partial v_{-2}}{\partial x_1} + 
  \frac{\partial v_1}{\partial v_{-1}}
  \frac{\partial v_{-1}}{\partial x_1} + 
  \frac{\partial v_1}{\partial v_{0}}
  \frac{\partial v_{0}}{\partial x_1} =
  \frac{\partial v_1}{\partial v_{-2}}
  1 + 
  \frac{\partial v_1}{\partial v_{-1}}
  0 + 
  \frac{\partial v_1}{\partial v_{0}}
  0 =
  1.
\end{align*}
The rest is calculated in the same manner, yielding the full evaluation trace as
\begin{align*}
  v_{-2} &= x_1 &&= 1 \\
  \dot{v}_{-2} &= \dot{x}_1 &&= 1 \\
  v_{-1} &= x_2 &&= 2 \\
  \dot{v}_{-1} &= \dot{x}_2 &&= 0 \\
  v_{0} &= x_3 &&= 3 \\
  \dot{v}_{0} &= \dot{x}_3 &&= 0 \\
  v_{1} &= v_{-2} + v_{-1} &&= 3 \\
  \dot{v}_{1} &= 1 &&= 1 \\
  v_{2} &= \cos(v_1) &&= -0.99 \\
  \dot{v}_{2} &= -\sin(v_1) &&= -0.83 \\
  v_{3} &= v_2 \cdot v_0 &&= -2.97 \\
  \dot{v}_{3} &= -v_0 \cdot \sin(v_1) &&= -2.51 \\
  y &= \cos(x_1 + x_2)x_3 &&= -2.97 \\
  \dot{y} &= -\sin(x_1 + x_2)x_3 &&= -0.41.
\end{align*}

#### Implementation
Below is an implementation of algorithmic differentiation (forward mode.

```{r create_ADnum_class}
# Make an algorithmic differentiation number
create_ADnum <- function(val, deriv = 1) {
  # a list that includes the number and its derivative
  x <- list(val = val, deriv = deriv)
  # make a class in R called 'ADnum'
  class(x) <- 'ADnum'
  return(x)
}

# Make the number 4 in a smart way
x <- create_ADnum(4)
x

# construct a custom print function for the 'ADnum' class
print.ADnum <- function(x, ...) {
  cat('value = ', x$val,
      ' and deriv = ', x$deriv, '\n', sep = '')
  return(invisible(x))
}
x
```

R knows how to calculate with regular numbers, but cannot yet calculate using number from the "ADnum" class.
For R to be able to do that we need to do what is known as operator overloading.

```{r operator_overloading}
Ops.ADnum <- function(e1, e2) {
  # Convert the first number to ADnum
  if (.Method[1] == '') {
    e1 <- create_ADnum(e1, 0)
  }
  # Convert the second number to ADnum
  if (.Method[2] == '') {
    e2 <- create_ADnum(e2, 0)
  }
  
  if (.Generic == '*') {
    return(create_ADnum(e1$val * e2$val, e1$deriv*e2$val + e2$deriv*e1$val))
  }
  
  if (.Generic == '+') {
    return(create_ADnum(e1$val + e2$val, e1$deriv + e2$deriv))
  }
  
  if (.Generic == '-') {
    return(create_ADnum(e1$val - e2$val, e1$deriv - e2$deriv))
  }
  
  if (.Generic == '/') {
    return(create_ADnum(e1$val / e2$val, (e1$deriv*e2$val - e1$val*e2$deriv)/e2$val^2))
  }
  
  stop("Function '", Generic, "' not yet implemented for ADnum")
}

Math.ADnum <- function(x, ...) {
  if (.Generic == "cos") {
    return(create_ADnum(cos(x$val), x$deriv * -sin(x$val)))
  } else if (.Generic == "sin") {
    return(create_ADnum(sin(x$val), x$deriv * cos(x$val)))
  } else if (.Generic == "exp") {
    return(create_ADnum(exp(x$val), x$deriv * exp(x$val)))
  }
  stop("Function '", .Generic, "' not yet implemented for Ad num")
}
```

Using the example from earlier we will now check that using our implementation we will in fact gain the same result.

```{r}
x <- create_ADnum(1); cos(x + 2)*3
```

We do in fact get the same result using our implementation of algorithmic differentiation.
\newpage