---
output:
  pdf_document: default
  html_document: default
---
# Least Squares
In least-squares problems, the objective have the following special form:
\begin{align}
  f(x) = \frac{1}{2}\sum_{j = 1}^m r^2_j(x).
\end{align}
We call $r_j$ a residual.
Furthermore we now that it is a smooth function from $\mathbb{R}^n$ to $\mathbb{R}$.
The special form of $f$ makes this particular type of problem easier to solve than general unconstrained minimization problems.

## Linear least-squares problems
If the model $\phi(x;t_j)$ is a linear function of $x$ what we have is a linear least-squares problem.
The residual is given by $r(x) = Jx - y$, where we call $J$ the design matrix.
The objective function becomes
\begin{align}\label{eq:objective_function}
  f(x) = \frac{1}{2}\|Jx - y\|^2.
\end{align}
The derivative and second derivative is given by
\begin{align}
  \nabla f(x) &= J^\top\left(Jx - y\right) \\
  \nabla^2 f(x) &= J^\top J.
\end{align}
The objective function \eqref{eq:objective_function} is convex and therefore theorem 2.5 states that: any $x^* \ : \ \nabla f(x^*) = 0$ also the global minimizer of $f$.

Knowing this we can deduce that
\begin{align}
  \nabla f(x) &= J^\top\left(Jx - y\right) \ \text{and} \ \nabla f(x^*) = 0 \\
  0 &= f(x^*) = J^\top\left(Jx^* - y\right) = J^\top Jx^* - J^\top y.
\end{align}
This means that $x^*$ satisfies the linear system of equations
\begin{align}\label{eq:normal_equations}
  J^\top Jx^* - J^\top y,
\end{align}
which are also known as the normal equations.
So to solve this unconstrained linear least squares problem, is equivalent to solving the set of normal equations given by \eqref{eq:normal_equations}.

## Algorithms for solving
There are three major algortihms for solving linear least squares problem, which we will present in this chapter.
Furthermore for every method we will present an implementation in `R` and compare the performance of the different methods.

For testing purposes we will use the `cars` dataset which we will now import.

```{r import_cars_dataset}
spd <- cars$speed
dst <- cars$dist
```

We will then define the design matrix using the `cars` data.

```{r}
j.1 <- rep(1, length(spd))
j.2 <- spd
J <- cbind(j.1, j.2)
y <- dst
```

### Cholesky / Normal equations
We start out by noting that
\begin{align}
  z^\top J^\top J z = \left(Jz\right)^\top Jz = \|Jz\|^2 > 0
\end{align}
as $Jz \neq 0$ when we assume that $J$ has full column rank.
This implies that $J^\top J$ is positive definite.
Therefore we can use Cholesky to factorise a matrix $A$ to lower triangular $L$ and $L$'s conjugate transpose, $\overline{L}^\top$.
Then solving the equations with a triangular would be easy by back substitution.

We can rephrase the problem by replacing
\begin{align}
  &J^\top Jx^* - J^\top y \ \text{with} \ Ax = b \\
  &\text{where} \ A = J ^\top J \ \text{and} \ b = J^\top y.
\end{align}
We can then use the following three step procedure to solve the system
\begin{itemize}
  \item compute the coefficient matrix $A = J^\top J$;
  \item compute the cholesky factorization of the symmetric matrix $J^\top J$;
  \item perform two triangular substitutions with the Cholesky factors to recover the solution $x^*$.
\end{itemize}
The two triangular substitutions are as follows
\begin{align}
  &\text{solve for $z$:} \ Lz = b \\
  &\text{solve for $x$:} \ L^\top x = z.
\end{align}

Below is a implementation of the procedure.

```{r}
# Compute A matrix by multiplying the transpose of J with J 
A <- t(J)%*%J
# Using the chol function to find the upper triangular cholesky factorization
U <- chol(A)
L <- t(chol(A))
# Computing the left side of the equation, by multiplying x transpose with y
b <- t(J)%*%y
# Using first the forwardsolve to find z
z <- forwardsolve(L, b)
# Hereafter using backsolve to find the coefficients
chol_coef <- backsolve(U, z)
# We can make a chol function for testing purposes
chol.est <- function(J) {
  A <- t(J)%*%J
  U <- chol(A)
  L <- t(U)
  b <- t(J)%*%y
  z <- forwardsolve(L, b)
  chol_coef <- backsolve(U, z)
  return(chol_coef)
}
chol.est(J)
```

### QR
Another method is using $QR$ factorization.
We can decompose $J = QR$ for orthogonal $Q$ and upper triangular $R$.
Assuming $J$ is square we have
\begin{align*}
  x^* &= \left(J^\top J\right)^{-1}J^\top y = \left(\left(QR\right)^\top\left(QR\right)\right)^{-1}\left(QR\right)^\top y\\
  &= \left(R^\top Q^\top Q R\right)^{-1}R^\top Q^\top y \\
  &= \left(R^\top R\right)^{-1} R^\top Q^\top y \\
  &= R^{-1}(R^\top)^{-1}R^\top Q^\top y \\
  &= R^{-1}Q^\top y.
\end{align*}
Therefore we can solve least squares by solving
\begin{align}
  Rx^* = Q^\top y
\end{align}
using back substitution as $R$ is triangular.

Below is an implementation of the procedure.

```{r}
# First we make the qr decomposition
QR <- qr(J)
# Then we utilize functions qr.Q and qr.R to extract the corresponding matrices
Q <- qr.Q(QR)
R <- qr.R(QR)
# As the equation we want to solve is Rx*=Q^T*y and R 
# is upper triangular we can use the backsolve function
QR_coef <- backsolve(R, t(Q)%*%y)
# We can now make a QR function for testing purposes
qr.est <- function (J) {
  QR <- qr(J)
  Q <- qr.Q(QR)
  R <- qr.R(QR)
  QR_coef <- backsolve(R, t(Q)%*%y)
  return(QR_coef)
}
qr.est(J)
```

### SVD
This third approach is based on the singular-value decomposition of $J$, it argues that $J = USV^\top$ for orthogonal $U$, $V$ and diagonal $S$.
It describes the effect of a matrix on a vector $Jx = USV^\top x$ as
\begin{itemize}
  \item a rotation/reflection in the input space ($V^\top$)
  \item a scaling that takes a vector in the input space to the output space $S$
  \item another rotation/reflection in the output space ($U$)
\end{itemize}
Assuming $J$ is square we have
\begin{align*}
  x^* &= \left(J^\top J\right)^{-1} J^\top y \\
  &= \left((USV^\top)^\top (USV^\top)\right)^{-1} (USV^\top)^\top y \\
  &= \left((VSU^\top) (USV^\top)\right)^{-1} (VSU^\top)y \\
  &= \left(VS^2V^\top\right)^{-1}(VSU^\top)y \\
  &= (V^\top)^{-1}S^{-2}V^{-1} (VSU^\top)y = (V^\top)^{-1} S^{-1}U^\top y \\
  &= VS^{-1}U^\top y.
\end{align*}
When $m > n$, similar to $QR$, then
\begin{align}
  x^* = VS^{-1}U^\top y
\end{align}

Below is an implementation of the procedure.

```{r}
# Perform SVD decomposition
svd. <- svd(J)
# Extract the three matrices from the decomposition
S <- diag(svd.$d)
U <- svd.$u
V <- svd.$v
# Solve the equation
svd_coef <- V%*%solve(S)%*%t(U)%*%y
# We can now make an SVD function for testing purposes
svd.est <- function (J) {
  svd. <- svd(J)
  S <- diag(svd.$d)
  U <- svd.$u
  V <- svd.$v
  svd_coef <- V%*%solve(S)%*%t(U)%*%y
  return(svd_coef)
}
```

## Comparison of the algortihms
The Cholesky based algorithm is particularly useful when $n < m$ and it is practical to store $J^\top J$ but not $J$ itself.
However the method can degrade if $J$ is ill-conditioned.
The QR based algortihm avoids squaring of the condition number and hence may be more robust that the Cholesky based method.
The SVD approach is the most robust of the three and even reveals information about the sesitivity of the solution to pertubation in the data.

We can now compare the performance of the three implementations, note that in addition to using the `qr.est` function we created above we also compare with the built-in function `qr.solve` as our function has added complexity for illustrative purposes.


```{r comparing_factorizations, include=F}
library(microbenchmark)
bench <- microbenchmark(Cholesky = chol.est(J), 
               QR = qr.est(J),
               'QR (Built-in)' = qr.solve(A, b),
               SVD = svd.est(J), 
               times = 5000)
library(dplyr)
bench <- bench %>%
  group_by(expr) %>%
  summarise(mean_time = mean(time)/1e+3)
```

```{r plotting_the_results_factorizations, echo = F, fig.height=3, fig.width=8}
library(ggplot2)
ggplot(bench) + 
  geom_col(aes(x = expr, y = mean_time), width = 0.5, fill = 'darkblue') +
  xlab('') + ylab('Mean time (microseconds)') + ggtitle('Comparing test results (5.000 runs)') +
  theme_minimal()
```

We can conclude that in our case there is a positive relationship between robustness and performance.
The Cholesky approach which is theoretically the least stable have the best performance, while the SVD approach which is theoretically the most stable have the worst performance if we disregard our implementaion of the QR method.
comparing the two QR implementations the built-in have an advantage when compared to our implementation.
We suspect that the discrepancy can be explained by our implementation doing some work twice.

\newpage