# Line search

## Exercise 1: Gradient descent

### What is the gradient of $f$?

### Implement gradient descent and then use it to find the best straight line

#### What is meant by *the best* straight line in relation to the objective function above

#### Discuss different ways to determine the step sizes

### Try with different ways to choose step sizes and illustrate it (including plotting the objective function and the iterates, $\{x_k\}_k$)

### Show some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`)

## Exercise 2: Stochastic gradient descent / incremental gradient descent

### What is the difference between stochastic gradient descent and gradient descent?

### How do you think the optimisation path (the path $\left (k, f(x_k) \right )$) looks like for stochastic gradient descent compared to that of the gradient descent?

### **Optional**: Implement stochastic gradient descent.

### **Optional**: Illustrate the behaviour of the stochastic gradient descent, including:

#### Different ways to choose step sizes.

#### The total objective function with a discussion of how it differs from a similar plot from the gradient descent method.

#### Some iterates in a plot showing the data (e.g. `plot(dist ~ speed, cars)`).

\newpage