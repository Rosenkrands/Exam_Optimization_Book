# Constrained Optimization

## Task 1
A boatyard produces three types of boats: cabin cruicers; racing sailboats and cruising sailboats. 
It takes 2 weeks to produce a cabin cruicer; 1 week to produce a racing sailboats and 3 weeks to produce a cruising sailboat. 
The boatyard is closed during the last week of December. 
The profit on each kind of boat is 5000 USD, 4000 USD and 6000 USD. 
Because of space considerations, the boatyard can finish at most 25 boats in one year (51 weeks).

To find the number of boats of each kind that will maximize the annual profit.
We wish to maximize the profitfunction.
\begin{align}
    Z(x) = 5000x_1 + 4000x_2 + 6000x_3
\end{align}
subject to the conditions that
\begin{align}
    2x_1 + 1x_2 + 3x_3 &\leq 51\\
    x_1 + x_2 + x_3 &\leq 25.
\end{align}
using the simplex method we find the optimal number of boats in the following way
\begin{table}[h]
  \begin{center}
    \begin{tabular}{@{}l|lllll|l@{}}
    \toprule
     & -5000 & -4000 & -6000 & 0 & 0 &    \\ \midrule
     & 2     & 1     & 3     & 1 & 0 & 51 \\
     & 1     & 1     & 1     & 0 & 1 & 25   
    \end{tabular}
  \end{center}
\end{table}
The last two columns represent the contraints.
The place where we can maximize profit the most is in the 3rd column.
We then have to choose whether we want pivot in the first or second row.
To determine this we divide the last column with the third to see where we have a bottle neck.
We conclude that the bottle neck is in the 1st row as $51/3 = 17 > 25/1 = 25$.
\begin{table}[h]
  \begin{center}
    \begin{tabular}{@{}l|lllll|l@{}}
    \toprule
     & -4000 & -2000 & 0 & 2000 & 0 &    \\ \midrule
     & 2/3   & 1/3   & 1 & 1/3  & 0 & 17 \\
     & 1/3   & 2/3   & 0 & -1/3 & 1 & 8  \\
    \end{tabular}
  \end{center}
\end{table}
We repeat the procedure again
\begin{table}[h]
  \begin{center}
    \begin{tabular}{@{}l|lllll|l@{}}
    \toprule
     & 0   & 0 & 0 & 1000 & 3000 &    \\ \midrule
     & 1/2 & 0 & 1 & -1/2 & 0    & 13 \\
     & 1/2 & 1 & 0 & -1/2 & 3/2  & 12 \\
    \end{tabular}
  \end{center}
\end{table}
and we get the result
\begin{align}
  x_1 = 0, x_2 = 12, x_3 = 13.
\end{align}
Inserting in the profitfunction yields
\begin{align}
    Z(x) = 12 \cdot 4000 \$ + 13 \cdot 6000 \$ = 126.0000 \$.
\end{align}

### Altering the constraints
The owner is looking to increase the profit and is considering various options.
\begin{itemize}
  \item Keeping the boatyard open all year round
  \item Increasing the capacity to 25 boasts
  \item Keeping the boatyard open all year round and icreasing the capacity to 26 boats
\end{itemize}

#### 52 weeks and 25 boats
Below we will use `R` to solve the optimization problem using the same technique as above.
First we will import the `MASS` library to display fractions for easier interpretation.
Next we will create a function that can pivot the matrix for a given matrix and pivot position.

```{r}
library(MASS)
pivot_matrix <- function(M, pivot){
  
  rp <- pivot[1]
  cp <- pivot[2]
  z <- (1:nrow(M))[-rp]
  
  M[rp,] <- M[rp,] / M[rp, cp]
  
  for (i in z){
    k <- M[i, cp]
    M[i, ] <- M[i,] - k* M[rp,]
  }
  M
}
```

We can now construct the matrix that corresponds to the optimization problem.

```{r}
M <- matrix(c(-5,1,2, -4,1,1, -6,1,3, 0,1,0, 0,0,1, 0,25,52), nr = 3); M
cp <- 3
M[-1, 6] / M[-1, cp]
rp <- 3
M2 <- pivot_matrix(M, c(rp,cp));fractions(M2)
cp <- 2
M2[-1, 6] / M2[-1, cp]
rp <- 2
M3 <- pivot_matrix(M2, c(2,2));M3
```

As we don't suspect anyone to buy half a boat we must choose which number we would round up.
If we construct 11 of the 2nd type and 14 of the third type we are able to store them but will use `r 11 * 1 + 14 * 3` weeks which we cannot.
Therefore we choose to construct 12 of the 2nd type and 13 of the 3rd type.
The profit in this case would amount to `r format(12 * 4000 + 13 * 6000, scientific = F)`.
Note that will only use `r 12 * 1 + 13 * 3` weeks.

#### 51 weeks and 26 boats

```{r}
M <- matrix(c(-5,1,2, -4,1,1, -6,1,3, 0,1,0, 0,0,1, 0,26,51), nr = 3); M
cp <- 3
M[-1, 6] / M[-1, cp]
rp <- 3
M2 <- pivot_matrix(M, c(rp,cp));fractions(M2)
cp <- 2
M2[-1, 6] / M2[-1, cp]
rp <- 2
M3 <- pivot_matrix(M2, c(2,2));M3
```

If we construct 13 of the 2nd type and 13 of the third type we are able to store them but will use `r 13 * 1 + 13 * 3` weeks which we cannot.
Therefore we choose to construct 14 of the 2nd type and 12 of the 3rd type.
The profit in this case would amount to `r format(14 * 4000 + 12 * 6000, scientific = F)`. 
Note that we will only use `r 14 * 1 + 12 * 3`

#### 52 weeks and 26 boats

```{r}
M <- matrix(c(-5,1,2, -4,1,1, -6,1,3, 0,1,0, 0,0,1, 0,26,52), nr = 3); M
cp <- 3
M[-1, 6] / M[-1, cp]
rp <- 3
M2 <- pivot_matrix(M, c(rp,cp));fractions(M2)
cp <- 2
M2[-1, 6] / M2[-1, cp]
rp <- 2
M3 <- pivot_matrix(M2, c(2,2));M3
```

In this case we can construct 13 of 2nd and 3rd type while using all weeks and storage capacity to achieve a profit of `r format(13 * 4000 + 13 * 6000, scientifuc = F)`.
We conclude that economically this is the best option.

## Task 2
What should the ratio between diameter and height of a cylinder, with a volume of 1 liter, be to use the smallest amount of material.
Solve the exercise using Lagrange multiplier technique.

The exercise can be rephrased to a optimization problem of the form: Minimize the objective function
\begin{align}
    A(r,h) = 2 \pi r(h + r)
\end{align}
subject to
\begin{align}
    V(r,h) = \pi r^2 h = 1. 
\end{align}
The lagrangian becomes
\begin{align}
    \mathcal{L}(r,h,\lambda) = 2 \pi r (h+r) - \lambda(\pi r^2 h - 1).
\end{align}
We then find the partial derivatives
\begin{align}
    \mathcal{L}'_{\lambda} &= - (\pi r^2 h - 1)\\
    \mathcal{L}'_r &= 2 \pi h + 4 \pi r - 2 \lambda \pi r h\\
    \mathcal{L}'_h &= 2 \pi r - \lambda \pi r^2
\end{align}
Setting the gradient equal to 0 gives the a system of 3 equations with 3 unknowns
\begin{align}
    \nabla \mathcal{L} = \begin{bmatrix}- (\pi r^2 h - 1)\\ 2 \pi h + 4 \pi r - 2 \lambda \pi r h\\ 2 \pi r - \lambda \pi r^2 \end{bmatrix}
    = \textbf{0}
\end{align}
The solution is
\begin{align}
    h  &\approx 1.0839 \\
    r  &\approx 0.5419 \\
    \lambda &\approx -3.6904.
\end{align}

## Task 3
Suppose the following situation: We have $n$ independent stochastic variables $y_1,\ldots,y_n$ where $y_i \sim N(\mu,\sigma^2v_i^2)$, and all $v_i$â€™s are known and $\sigma^2$ is unknown.

### Part 1
We wish to estimate $\mu$.
Let $\overline{y} = \dfrac{1}{n} \sum_i y_i$.
What is $E[\overline{y}]$ and $V[\overline{y}]$? Can we find a better estimate for $\mu$ than a simple average.

Let $\Bar{y}=\frac{1}{n}\sum_i y_i$.
We want to estimate the mean and variance.
We leave out $\sigma^2$ as this is an unknown constant.
\begin{align*}
    E(\Bar{y}) &= E(\frac{1}{n}\sum_i y_i) = \frac{1}{n}E(\sum_i y_i) = \frac{1}{n}n \mu = \mu \\
    V(\Bar{y}) &= \frac{1}{n^2} V(\sum_i y_i) = \frac{1}{n^2} \sum_i V(y_i) = \frac{1}{n^2} \sum_i v_i^2.
\end{align*}
One would be able to come up with a better estimate by weighting the data points with lowest variance higher, but still consider all $y_i$'s, as they still hold information about the true mean value.

### Part 2
Let $\tilde{y} = \sum_i p_i y_i$ where $p= (p_1,...,p_n)$ be a deterministic vector.
what value of $\textbf{p}$ gives $\tilde{y}$ the lowest possible variance?
What restriction should we put on $p_1,\ldots p_n$ if we want $E(\tilde{y})=\mu$?

First we calculate the variance
\begin{align*}
    V(\tilde{y}) = V(\sum_i p_i y_i) = \sum_i p_i^2 V(y_i) = \sum_i p_i^2 v_i^2
\end{align*}
The value for which we get the lowest variance is 0, however this is a trivial case that gets us no closer to the true mean.
Next we find the mean
\begin{align*}
    E(\tilde{y}) = E(\sum_i p_i y_i) = \sum_i p_i E(y_i) = \mu \sum_i p_i \Rightarrow \sum_i p_i = 1
\end{align*}
This tells us that the weights must sum to one.

### Part 3
In statistics we often wish to find a parameter estimate, that is unbiased and have the lowest possible variance. 
We wish to choose $p$ so $\tilde{y}$ have mean value $\mu$ and lowest possible variance. 
Solve the problem using the Lagrange multiplier method.

The problem can be formulated as the following Lagrange function
\begin{align*}
    L = \sum_i p_i^2 v_i^2 - \lambda(\sum_i p_i - 1)
\end{align*}
Setting the partial derivatives equal to zero yields
\begin{align*}
    \frac{\partial L}{\partial p_i} = 2p_i v_i^2 -\lambda = 0 \\
    \frac{\partial L}{\partial \lambda} = -\sum_i p_i + 1 = 0.
\end{align*}
Thus we have a system of $k+1$ equations with $k+1$ unknowns.
For $k=3$ we get the following system
\begin{align*}
    \begin{bmatrix}
        2 v_1^2 & 0 & 0 & -1 & 0 \\
        0 & 2v_2^2 & 0 & -1 & 0 \\
        0 & 0 & 2v_3^2 & -1 & 0 \\
        1 & 1 & 1 & 0 & 1
    \end{bmatrix}
    & \sim
    \begin{bmatrix}
        1 & 0 & 0 & \frac{-1}{2v_1^2} & 0 \\
        0 & 1 & 0 & \frac{-1}{2v_2^2} & 0 \\
        0 & 0 & 1 & \frac{-1}{2v_3^2} & 0 \\
        0 & 0 & 0 & \frac{1}{2v_1^2} + \frac{1}{2v_2^2} + \frac{1}{2v_3^2} & 1
    \end{bmatrix} \\
    & \sim 
    \begin{bmatrix}
        1 & 0 & 0 & 0 & \frac{\frac{1}{2v_1^2}}{\frac{1}{2v_1^2} + \frac{1}{2v_2^2} + \frac{-1}{2v_3^2}} \\
        0 & 1 & 0 & 0 & \frac{\frac{1}{2v_2^2}}{\frac{1}{2v_1^2} + \frac{1}{2v_2^2} + \frac{1}{2v_3^2}} \\
        0 & 0 & 1 & 0 & \frac{\frac{1}{2v_3^2}}{\frac{1}{2v_1^2} + \frac{1}{2v_2^2} + \frac{1}{2v_3^2}} \\
        0 & 0 & 0 & 1 & \frac{1}{\frac{1}{2v_1^2} + \frac{1}{2v_2^2} + \frac{1}{2v_3^2}}
    \end{bmatrix}.
\end{align*}
From this we get the equations
\begin{align*}
    \lambda &= \frac{1}{\frac{1}{2v_1^2} + \frac{1}{2v_2^2} + \frac{1}{2v_3^2}} \\
    p_i &= \frac{1}{2v_i^2} \lambda = \frac{\frac{1}{v_i^2}}{\sum_i \frac{1}{v_i^2}}
\end{align*}
The same solution applies for arbitrary values of $k\in\mathbb{N}$.

## Task 4
Consider a linear regression model of the form
\begin{align*}
  y_i = \beta_0 + \beta_1 x_i + e_i,
\end{align*}
with the constraint $\beta_1 > 0$.
In the chunk below we formulate and solve the problem using the `cars` dataset and the `CVXR` package.

```{r}
suppressWarnings(library(CVXR, warn.conflicts = FALSE))
p <- 2
X <- cbind(rep(1,length(cars$speed)),cars$speed)
Y <- cars$dist
betaHat <- Variable(p)
objective <- Minimize(sum((Y - X %*% betaHat)^2))
problem.1 <- Problem(objective)
problem.2 <- Problem(objective, constraints = list(betaHat[2] >= 0))
result.1 <- solve(problem.1)
result.2 <- solve(problem.2)
```

Below we extract the coefficients from the two formulations

```{r}
coef.1 <- result.1$getValue(betaHat)
coef.2 <- result.2$getValue(betaHat)
```

Below is a plot using the coeficients from the two problems.

```{r echo = F}
plot(cars$speed, cars$dist, xlim = c(0,30), xlab = 'Speed', ylab = 'Distance')
abline(coef.1[1,1], coef.1[2,1], col = 'green')
abline(coef.2[1,1], coef.2[2,1], col = 'red')
```

Next we can explore the problem using the `nls` function with the formulated in such a way that we don't need to impose the constraint.

```{r}
y <- cars$dist
x <- cars$speed
nl.fit.con <- nls(y ~ a + exp(b) * x, start = list(a = 1, b = 1))
nl.fit <- nls(y ~ a + b * x, start = list(a = 1, b = 1))
```

Below we can see the summaries form the two regressions.

```{r}
summary(nl.fit.con)
summary(nl.fit)
```